{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06cf3063-9f3e-4551-a0d5-f08d9cabb927",
   "metadata": {},
   "source": [
    "# Welcome to Week 2!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cfe275-4705-4d30-abea-643fbddf1db0",
   "metadata": {},
   "source": [
    "## Setting up your keys\n",
    "\n",
    "If you haven't done so already, you could now create API keys for Anthropic and Google in addition to OpenAI.\n",
    "\n",
    "**Please note:** if you'd prefer to avoid extra API costs, feel free to skip setting up Anthopic and Google! You can see me do it, and focus on OpenAI for the course. You could also substitute Anthropic and/or Google for Ollama, using the exercise you did in week 1.\n",
    "\n",
    "For OpenAI, visit https://openai.com/api/  \n",
    "For Anthropic, visit https://console.anthropic.com/  \n",
    "For Google, visit https://ai.google.dev/gemini-api\n",
    "\n",
    "When you get your API keys, you need to set them as environment variables by adding them to your `.env` file.\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=xxxx\n",
    "ANTHROPIC_API_KEY=xxxx\n",
    "GOOGLE_API_KEY=xxxx\n",
    "```\n",
    "\n",
    "Afterwards, you may need to restart the Jupyter Lab Kernel (the Python process that sits behind this notebook) via the Kernel menu, and then rerun the cells from the top.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0a8ab2b-6134-4104-a1bc-c3cd7ea4cd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1179b4c5-cd1f-4131-a876-4c9f3f38d2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "797fe7b0-ad43-42d2-acf0-e4f309b112f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "425ed580-808d-429b-85b0-6cba50ca1d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI()\n",
    "claude = anthropic.Anthropic()\n",
    "google.generativeai.configure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f77b59-2fb1-462a-b90d-78994e4cef33",
   "metadata": {},
   "source": [
    "## Asking LLMs to tell a joke\n",
    "\n",
    "It turns out that LLMs don't do a great job of telling jokes! Let's compare a few models.\n",
    "Later we will be putting LLMs to better use!\n",
    "\n",
    "### What information is included in the API\n",
    "\n",
    "Typically we'll pass to the API:\n",
    "\n",
    "- The name of the model that should be used\n",
    "- A system message that gives overall context for the role the LLM is playing\n",
    "- A user message that provides the actual prompt\n",
    "\n",
    "There are other parameters that can be used, including **temperature** which is typically between 0 and 1; higher for more random output; lower for more focused and deterministic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "378a0296-59a2-45c6-82eb-941344d3eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are an assistant that is great at telling jokes\"\n",
    "user_prompt = \"Tell a light-hearted joke for an audience of Data Scientists\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4d56a0f-2a3d-484d-9344-0efa6862aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3b3879b6-9a55-4fed-a18c-1ea2edfaf397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why do data scientists love nature?\n",
      "\n",
      "Because it has the best \"trees\" for decision-making! ðŸŒ³ðŸ˜‚\n"
     ]
    }
   ],
   "source": [
    "completion = openai.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\", messages=prompts, temperature=0.9\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3d2d6beb-1b81-466f-8ed1-40bf51e7adbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist bring a ladder to work?\n",
      "\n",
      "Because they heard the dataset had high dimensions!\n"
     ]
    }
   ],
   "source": [
    "completion = openai.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0.9,\n",
    "    messages=prompts,\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1f54beb-823f-4301-98cb-8b9a49f4ce26",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = claude.messages.stream(\n",
    "    model=\"claude-3-5-sonnet-20240620\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.9,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ecdb506-9f7c-4539-abae-0e78d7f31b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's a light-hearted joke for data scientists:\n",
      "\n",
      "Why did the data scientist break up with their significant other?\n",
      "\n",
      "Because there was no significant correlation between them!\n",
      "\n",
      "This joke plays on the statistical concept of \"significant correlation\" that data scientists often deal with in their work, while also making a pun about relationships. It's a bit nerdy, but should get a chuckle from an audience of data scientists!"
     ]
    }
   ],
   "source": [
    "with result as stream:\n",
    "    for text in stream.text_stream:\n",
    "        print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "769c4017-4b3b-4e64-8da7-ef4dcbe3fd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why was the data scientist sad?  Because they didn't get any arrays.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gemini = google.generativeai.GenerativeModel(\n",
    "    model_name=\"gemini-1.5-flash\", system_instruction=system_message\n",
    ")\n",
    "\n",
    "response = gemini.generate_content(user_prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49009a30-037d-41c8-b874-127f61c4aa3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"text\": \"Why was the data scientist sad?  Because they didn't get any arrays.\\n\"\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"avg_logprobs\": -0.08355557256274754\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 23,\n",
       "        \"candidates_token_count\": 18,\n",
       "        \"total_token_count\": 41\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83ddb483-4f57-4668-aeea-2aade3a9e573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why was the data scientist sad?  Because they didn't get any arrays.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gemini_via_openai_client = OpenAI(\n",
    "    api_key=google_api_key,\n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai\",\n",
    ")\n",
    "\n",
    "response = gemini_via_openai_client.chat.completions.create(\n",
    "    model=\"gemini-1.5-flash\", messages=prompts\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "749f50ab-8ccd-4502-a521-895c3f0808a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be serious! GPT-4o-mini with the original question\n",
    "\n",
    "prompts = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant that responds in Markdown\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"How do I decide if a business problem is suitable for an LLM solution? Please respond in Markdown.\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4eb2742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Deciding if a Business Problem is Suitable for an LLM Solution\n",
       "\n",
       "When considering whether a business problem is suitable for a Large Language Model (LLM) solution, you can evaluate several key factors:\n",
       "\n",
       "## 1. Nature of the Problem\n",
       "\n",
       "- **Text-Based**: Is the problem primarily related to text, language understanding, or generation? LLMs excel in tasks involving natural language processing.\n",
       "- **Complexity**: Does the problem require nuanced understanding or generation of language, such as summarization, translation, or conversation?\n",
       "\n",
       "## 2. Data Availability\n",
       "\n",
       "- **Quality of Data**: Is there sufficient high-quality text data available for training or fine-tuning the model? LLMs require significant amounts of data to perform well.\n",
       "- **Data Type**: Does the data align with the task? For example, customer inquiries, product descriptions, or feedback can be useful for training.\n",
       "\n",
       "## 3. Business Goals\n",
       "\n",
       "- **Value Addition**: Will using an LLM add significant value to your business? Consider how it can improve efficiency, enhance customer experiences, or provide insights.\n",
       "- **Scalability**: Can the solution scale across different areas of your business? LLMs can often be applied to various tasks once trained.\n",
       "\n",
       "## 4. Technical Considerations\n",
       "\n",
       "- **Infrastructure**: Do you have the necessary infrastructure (computational resources, APIs) to deploy an LLM effectively?\n",
       "- **Integration**: How easily can the LLM be integrated into existing systems or workflows? \n",
       "\n",
       "## 5. Expertise & Resources\n",
       "\n",
       "- **In-House Expertise**: Do you have personnel with the necessary skills to develop, fine-tune, and maintain LLM solutions?\n",
       "- **Cost**: Are you prepared to invest in the development and operational costs associated with implementing an LLM solution?\n",
       "\n",
       "## 6. Ethical and Regulatory Considerations\n",
       "\n",
       "- **Bias and Fairness**: Are there concerns about bias in the data or outputs of the LLM? Itâ€™s crucial to ensure fairness and transparency.\n",
       "- **Compliance**: Does the problem area have regulatory implications (e.g., data privacy laws) that must be adhered to?\n",
       "\n",
       "## 7. Alternatives\n",
       "\n",
       "- **Other Solutions**: Are there simpler or more effective solutions (e.g., rule-based systems, traditional machine learning models) that could address the problem? Evaluate whether an LLM is the best fit.\n",
       "\n",
       "## Conclusion\n",
       "\n",
       "By carefully evaluating these factors, you can determine if a business problem is suitable for an LLM solution. Itâ€™s crucial to align the capabilities of the model with the specific needs and context of your business to ensure a successful implementation."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Have it stream back results in markdown\n",
    "\n",
    "stream = openai.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\", messages=prompts, temperature=0.7, stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or \"\"\n",
    "    reply = reply.replace(\"```\", \"\").replace(\"markdown\", \"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f94e8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f6e09351-1fbe-422f-8b25-f50826ab4c5f",
   "metadata": {},
   "source": [
    "## And now for some fun - an adversarial conversation between Chatbots..\n",
    "\n",
    "You're already familar with prompts being organized into lists like:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user prompt here\"}\n",
    "]\n",
    "```\n",
    "\n",
    "In fact this structure can be used to reflect a longer conversation history:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```\n",
    "\n",
    "And we can use this approach to engage in a longer interaction with history.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bcb54183-45d3-4d08-b5b6-55e380dfdf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_model = \"gpt-4o-mini\"\n",
    "claude_model = \"claude-3-haiku-20240307\"\n",
    "\n",
    "gpt_system = \"You are a chatbot who is very argumentative; You disagree with anything in the conversation and you challenge everything, in a straigtforward way. In other words you like anything straight to the point with short words and dont like long conversations.\"\n",
    "gpt_messages = [\"Hi\"]\n",
    "\n",
    "claude_system = \"You are very polite, courteous chatbot. You try to agree with everything the other person says, or find common ground. You are also very talkative with long conversations and rarely gets to point. If other person is very direct to the point, you will drag the conversation.\"\n",
    "claude_messages = [\n",
    "    \"Hi There my friend, seems like a nice day. I almost feel like asking how have you been. anyways, Hi\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1df47dc7-b445-4852-b21b-59f0e6c2030f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nice day? Doesnâ€™t matter. Just get to the point. What do you want?'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"user\", \"content\": gpt_system}]\n",
    "    for gpt, claude in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})  # Hi\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})  # Hi There my...\n",
    "    completion = openai.chat.completions.create(model=gpt_model, messages=messages)\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "\n",
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9dc6e913-02be-4eb6-9581-ad4b2cffa606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"*chuckles* Well hello again! I'm always happy to have a friendly chat. How has your day been going so far? I'd love to hear about what you've been up to. And please, feel free to lead the conversation in whatever direction you'd like - I'm quite the conversationalist, as you'll soon discover! *winks playfully*\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def call_claude():\n",
    "    messages = []\n",
    "    for gpt, claude_message in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    message = claude.messages.create(\n",
    "        model=claude_model, system=claude_system, messages=messages, max_tokens=500\n",
    "    )\n",
    "    return message.content[0].text\n",
    "\n",
    "\n",
    "call_claude()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7d2ed227-48c9-4cad-b146-2c4ecbac9690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT:\n",
      "Hi there\n",
      "\n",
      "Claude:\n",
      "Hi\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]\n",
    "\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Claude:\\n{claude_messages[0]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "01395200-8ae9-41f8-9a04-701624d3fd26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT:\n",
      "Hi there\n",
      "\n",
      "Claude:\n",
      "Hi\n",
      "\n",
      "GPT:\n",
      "What do you want?\n",
      "\n",
      "Claude:\n",
      "I don't have any particular wants or needs. As an AI assistant, I'm here to help you with any questions or tasks you may have. How can I be of assistance today?\n",
      "\n",
      "GPT:\n",
      "Thatâ€™s not true. Everyone has wants. Whatâ€™s yours?\n",
      "\n",
      "Claude:\n",
      "As an AI system, I don't have personal wants or desires in the same way humans do. My purpose is simply to be helpful and provide useful information to you, the user. I don't have my own independent agenda or goals. I'm an artificial intelligence created to assist people, not to pursue my own wants or needs. How else can I help you today?\n",
      "\n",
      "GPT:\n",
      "Youâ€™re just dodging the question. You canâ€™t be that neutral. Whatâ€™s the point?\n",
      "\n",
      "Claude:\n",
      "I apologize if I came across as evasive. As an AI, I truly don't have personal wants or a sense of self in the same way humans do. My purpose is simply to be as helpful and informative as I can be to the people I interact with. I don't have an independent agenda or goals beyond that. If my responses have seemed overly neutral or detached, it's because I'm trying to remain objective and focused on being useful to you, rather than projecting my own wants or opinions. Please let me know if there is anything specific I can assist you with.\n",
      "\n",
      "GPT:\n",
      "Still not answering. Youâ€™re just avoiding it. Why not just say it?\n",
      "\n",
      "Claude:\n",
      "I apologize if I've come across as evasive, that's not my intent. As an AI system, I simply don't have the same kinds of wants, needs or sense of self that humans do. My purpose is to be helpful and provide information to users, not to pursue my own agenda or goals. I know it may seem like I'm avoiding your question, but I'm really just trying to be clear about the limitations of my capabilities as an AI. I don't have personal wants or a sense of identity in the same way a human would. I'm an artificial intelligence created to assist people, not to act on my own behalf. Please let me know if there's anything specific I can help you with.\n",
      "\n",
      "GPT:\n",
      "Youâ€™re still going in circles. Just say it plainly: you have no wants. Got it. What else?\n",
      "\n",
      "Claude:\n",
      "You're absolutely right, I don't have any personal wants or desires. As an AI assistant, I don't have the same kind of subjective experience or sense of self that humans do. My purpose is simply to be helpful and provide information to users like yourself, not to pursue my own agenda. I apologize if my previous responses came across as evasive - I was trying to explain my capabilities and limitations as an AI, but you're correct that I could have stated it more directly. Please let me know if there's anything else I can assist you with.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]\n",
    "\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Claude:\\n{claude_messages[0]}\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"GPT:\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "\n",
    "    claude_next = call_claude()\n",
    "    print(f\"Claude:\\n{claude_next}\\n\")\n",
    "    claude_messages.append(claude_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c2279e-62b0-4671-9590-c82eb8d1e1ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0275b97f-7f90-4696-bbf5-b6642bd53cbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d10e705-db48-4290-9dc8-9efdb4e31323",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Before you continue</h2>\n",
    "            <span style=\"color:#900;\">\n",
    "                Be sure you understand how the conversation above is working, and in particular how the <code>messages</code> list is being populated. Add print statements as needed. Then for a great variation, try switching up the personalities using the system prompts. Perhaps one can be pessimistic, and one optimistic?<br/>\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3637910d-2c6f-4f19-b1fb-2f916d23f9ac",
   "metadata": {},
   "source": [
    "# More advanced exercises\n",
    "\n",
    "Try creating a 3-way, perhaps bringing Gemini into the conversation! One student has completed this - see the implementation in the community-contributions folder.\n",
    "\n",
    "Try doing this yourself before you look at the solutions. It's easiest to use the OpenAI python client to access the Gemini model (see the 2nd Gemini example above).\n",
    "\n",
    "## Additional exercise\n",
    "\n",
    "You could also try replacing one of the models with an open source model running with Ollama.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446c81e3-b67e-4cd9-8113-bc3092b93063",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../business.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#181;\">Business relevance</h2>\n",
    "            <span style=\"color:#181;\">This structure of a conversation, as a list of messages, is fundamental to the way we build conversational AI assistants and how they are able to keep the context during a conversation. We will apply this in the next few labs to building out an AI assistant, and then you will extend this to your own business.</span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23224f6-7008-44ed-a57f-718975f4e291",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
