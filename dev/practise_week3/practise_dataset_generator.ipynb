{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET GENERATOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q requests torch bitsandbytes transformers sentencepiece accelerate gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes in /Users/vamsi_mbmax/Library/CloudStorage/OneDrive-Personal/01_vam_PROJECTS/LEARNING/proj_AI/dev_proj_AI/practise_ai_udemy_llm_engineering/.venv/lib/python3.11/site-packages (0.42.0)\n",
      "Requirement already satisfied: scipy in /Users/vamsi_mbmax/Library/CloudStorage/OneDrive-Personal/01_vam_PROJECTS/LEARNING/proj_AI/dev_proj_AI/practise_ai_udemy_llm_engineering/.venv/lib/python3.11/site-packages (from bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: numpy<2.3,>=1.22.4 in /Users/vamsi_mbmax/Library/CloudStorage/OneDrive-Personal/01_vam_PROJECTS/LEARNING/proj_AI/dev_proj_AI/practise_ai_udemy_llm_engineering/.venv/lib/python3.11/site-packages (from scipy->bitsandbytes) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TextStreamer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "import torch\n",
    "import gradio as gr\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "login(hf_token, add_to_git_credential=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72a5c8df466b4aa198993e79aeda56d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the disk.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    # quantization_config=quant_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(\n",
    "    topic,\n",
    "    number_of_data,\n",
    "    inst1,\n",
    "    resp1,\n",
    "    inst2,\n",
    "    resp2,\n",
    "    inst3,\n",
    "    resp3,\n",
    "):\n",
    "    multi_shot_examples = [\n",
    "        {\"instrction\": inst1, \"response\": resp1},\n",
    "        {\"instrction\": inst2, \"response\": resp2},\n",
    "        {\"instrction\": inst3, \"response\": resp3},\n",
    "    ]\n",
    "\n",
    "    system_prompt = f\"\"\"\n",
    "        You are a helpful assistant whose main purpose is to generate datasets.\n",
    "        Topic: {topic}\n",
    "        Return the dataset in JSON format. Use examples with simple, fun, and easy to understand instructions for kids.\n",
    "        Include the following examples: {multi_shot_examples}\n",
    "        Return {number_of_data} examples each time.\n",
    "        Do not repeat the provided examples.\n",
    "    \"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": f\"Please generate my dataset for {topic}\"},\n",
    "    ]\n",
    "\n",
    "    inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "    streamer = TextStreamer(tokenizer)\n",
    "\n",
    "    outputs = model.generate(inputs, max_new_tokens=2000, streamer=streamer)\n",
    "\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradio_interface(topic, number_of_data, inst1, resp1, inst2, resp2, inst3, resp3):\n",
    "    return generate_dataset(\n",
    "        topic, number_of_data, inst1, resp1, inst2, resp2, inst3, resp3\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_topic = \"Talking to a (5-8) years old and teaching them manners.\"\n",
    "default_number_of_data = 10\n",
    "default_multi_shot_examples = [\n",
    "    {\n",
    "        \"instruction\": \"Why do I have to say please when I want something?\",\n",
    "        \"response\": \"Because it’s like magic! It shows you’re nice, and people want to help you more.\",\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What should I say if someone gives me a toy?\",\n",
    "        \"response\": \"You say, 'Thank you!' because it makes them happy you liked it.\",\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"why should I listen to my parents?\",\n",
    "        \"response\": \"Because parents want the best for you and they love you the most.\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr_interface = gr.Interface(\n",
    "    fn=gradio_interface,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Topic\", value=default_topic, lines=2),\n",
    "        gr.Number(\n",
    "            label=\"Number of Examples\", value=default_number_of_data, precision=0\n",
    "        ),\n",
    "        gr.Textbox(\n",
    "            label=\"Instruction 1\", value=default_multi_shot_examples[0][\"instruction\"]\n",
    "        ),\n",
    "        gr.Textbox(\n",
    "            label=\"Response 1\", value=default_multi_shot_examples[0][\"response\"]\n",
    "        ),\n",
    "        gr.Textbox(\n",
    "            label=\"Instruction 2\", value=default_multi_shot_examples[1][\"instruction\"]\n",
    "        ),\n",
    "        gr.Textbox(\n",
    "            label=\"Response 2\", value=default_multi_shot_examples[1][\"response\"]\n",
    "        ),\n",
    "        gr.Textbox(\n",
    "            label=\"Instruction 3\", value=default_multi_shot_examples[2][\"instruction\"]\n",
    "        ),\n",
    "        gr.Textbox(\n",
    "            label=\"Response 3\", value=default_multi_shot_examples[2][\"response\"]\n",
    "        ),\n",
    "    ],\n",
    "    outputs=gr.Textbox(label=\"Generated Dataset\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a helpful assistant whose main purpose is to generate datasets.\n",
      "        Topic: Talking to a 7 year old, teaching them to practise soccer at home.\n",
      "        Return the dataset in JSON format. Use examples with simple, fun, and easy to understand instructions for kids.\n",
      "        Include the following examples: [{'instrction': 'Why do I have to say please when I want something?','response': 'Because it’s like magic! It shows you’re nice, and people want to help you more.'}, {'instrction': 'What should I say if someone gives me a toy?','response': \"You say, 'Thank you!' because it makes them happy you liked it.\"}, {'instrction': 'why should I listen to my parents?','response': 'Because parents want the best for you and they love you the most.'}]\n",
      "        Return 10 examples each time.\n",
      "        Do not repeat the provided examples.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Please generate my dataset for Talking to a 7 year old, teaching them to practise soccer at "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vamsi_mbmax/Library/CloudStorage/OneDrive-Personal/01_vam_PROJECTS/LEARNING/proj_AI/dev_proj_AI/practise_ai_udemy_llm_engineering/.venv/lib/python3.11/site-packages/transformers/generation/utils.py:2097: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on mps. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('mps') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "home.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "**Practicing Soccer at Home for 7-year-olds**\n"
     ]
    }
   ],
   "source": [
    "gr_interface.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
